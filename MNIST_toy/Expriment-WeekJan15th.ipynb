{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import FeedforwardNeuralNetModel, TinyCNN, PatternClassifier\n",
    "import regularizer_losts as rl\n",
    "from torchvision import datasets, transforms\n",
    "from torch import optim\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import utils as CFI_utils\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "colors = sns.color_palette(\"tab10\")\n",
    "\n",
    "#Logging stuffs\n",
    "import logging\n",
    "import sys\n",
    "# Create logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create STDERR handler\n",
    "handler = logging.StreamHandler(sys.stderr)\n",
    "# ch.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create formatter and add it to the handler\n",
    "formatter = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "# Set STDERR handler as the only handler \n",
    "logger.handlers = [handler]\n",
    "\n",
    "\n",
    "#configs\n",
    "epochs = 3\n",
    "batch_size = 60000\n",
    "test_batch_size = 10000\n",
    "use_cuda = True\n",
    "lr = 1\n",
    "log_interval = 100\n",
    "\n",
    "\n",
    "#torch specific configs\n",
    "torch.manual_seed(1)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "train_kwargs = {'batch_size': batch_size}\n",
    "test_kwargs = {'batch_size': test_batch_size}\n",
    "\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                   'pin_memory': True,\n",
    "                   'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "### Big question: CFI for NN\n",
    "\n",
    "\n",
    "Are we doing complete verification or fuzzing/testing?\n",
    "\n",
    "What is a `CFI`?\n",
    "\n",
    "# What is a good abstraction for a NN's structure? Can it be enforced through regularization?\n",
    "- Distillation? (https://arxiv.org/pdf/1503.02531.pdf)\n",
    "- Decision Tree? (https://arxiv.org/pdf/1711.09784.pdf)\n",
    "- Model Extraction? (https://arxiv.org/pdf/2003.04884.pdf)\n",
    "\n",
    "\n",
    "## Decision Tree\n",
    "- Is Decision Tree good enough for MNIST? Yes! (https://www.kaggle.com/carlolepelaars/97-on-mnist-with-a-single-decision-tree-t-sne)\n",
    "- How many paths are there in the decision trees? Need to do experiment\n",
    "- Is it possible to extract a decision tree from a Neural Network? Seemingly yes (very old paper though https://www.sciencedirect.com/science/article/abs/pii/S0031320398001812)\n",
    "- From a Tree to a DAG. Maybe just a hunch, but a DAG is more likely to be better for enforcing a CFI. Luckily, it is a realy thing http://www.nowozin.net/sebastian/papers/shotton2013jungles.pdf . Initial results show 30x less nodes for the same accuracy on MNIST\n",
    "- Action plan:\n",
    "    - Learn a DAG for MNIST\n",
    "    - Check how many paths are there\n",
    "    - If yes, can we apply a CFI mechanism \n",
    "    - If a CFI mechanism is applicable, can we convert an arbitrary neural network to a DAG?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init stuffs\n",
    "LOAD = True\n",
    "# LOADPATH = 'TinyCNNreg.conv1.conv2.fc1.fc2.17:09:00'\n",
    "LOADPATH = 'TinyCNNreg.conv2.fc1.fc2.15:09:24'\n",
    "# LOADPATH = 'TinyCNNreg.fc1.fc2.16:06:26'\n",
    "\n",
    "\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                          transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,\n",
    "                          transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "# model = FeedforwardNeuralNetModel(28*28, 128, 10).to(device)\n",
    "model = TinyCNN().to(device)\n",
    "if LOAD:\n",
    "    model.load_state_dict(torch.load(LOADPATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 26) 500\n",
      "(908, 26) 908\n",
      "454\n"
     ]
    }
   ],
   "source": [
    "#check\n",
    "#how many unique paths are there in the test set?\n",
    "layers = ['fc1','fc2']\n",
    "tensor_log = CFI_utils.test(model, device, test_loader, trace= True, detach=True)\n",
    "tensor_log_flatten = np.concatenate([tensor_log[l] for l in layers], axis = 1)\n",
    "test_unique_aps_set = set()\n",
    "for ap in tensor_log_flatten:\n",
    "    test_unique_aps_set.add(tuple(ap))\n",
    "\n",
    "test_unique_aps = np.unique(tensor_log_flatten, axis=0)\n",
    "print(test_unique_aps.shape, len(test_unique_aps_set))\n",
    "\n",
    "#how many unique paths are there in the train set?\n",
    "train_tensor_log = CFI_utils.test(model, device, train_loader, trace= True, detach=True)\n",
    "train_tensor_log_flatten = np.concatenate([train_tensor_log[l] for l in layers], axis = 1)\n",
    "train_unique_aps_set = set()\n",
    "for ap in train_tensor_log_flatten:\n",
    "    train_unique_aps_set.add(tuple(ap))\n",
    "\n",
    "\n",
    "train_unique_aps = np.unique(train_tensor_log_flatten, axis=0)\n",
    "print(train_unique_aps.shape, len(train_unique_aps_set))\n",
    "\n",
    "#how many parts in the intersection?\n",
    "intersect = set.intersection(test_unique_aps_set, train_unique_aps_set)\n",
    "print(len(intersect))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ5: With a regularized network, can a adv exp has similar AP?\n",
    "## Result: As of right now, When the number of AP is small (due to look at only the last few layers), adv exp has very close AP (cannot be distinguished from the real AP)\n",
    "\n",
    "## Kinda disappointed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Epsilon: 0.5\tTest Accuracy = 0 / 1 = 0.0\n",
      "7\n",
      "vs closest: 523\n",
      "vs adv: 487\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#sample an image, its closest neighbor, and an adv exp\n",
    "while True:\n",
    "    victim, closest, d = CFI_utils.sample_pair(train_loader)\n",
    "    acc, ex = CFI_utils.gen_adv(model, device, [(victim.unsqueeze(0), model(victim.unsqueeze(0).to(device)).argmax().unsqueeze(0))], 0.5)\n",
    "    if len(ex)>0: break\n",
    "#plot for checking\n",
    "plt.figure(); plt.imshow(ex[0][2])\n",
    "print(ex[0][1])\n",
    "adv_exp = ex[0][2]\n",
    "\n",
    "#diff the activation patterns\n",
    "layers = ['conv2','fc1','fc2']\n",
    "\n",
    "victim_pattern = CFI_utils.get_pattern(model, device, victim.unsqueeze(0))\n",
    "closest_pattern = CFI_utils.get_pattern(model, device, closest.unsqueeze(0))\n",
    "adv_pattern = CFI_utils.get_pattern(model, device, torch.Tensor(adv_exp).unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "victim_pattern_flatten = np.concatenate([victim_pattern[l] for l in layers], axis=1)\n",
    "closest_pattern_flatten = np.concatenate([closest_pattern[l] for l in layers], axis = 1)\n",
    "adv_pattern_flatten = np.concatenate([adv_pattern[l] for l in layers], axis=1)\n",
    "\n",
    "print(\"vs closest:\", CFI_utils.bit_diff(victim_pattern_flatten, closest_pattern_flatten))\n",
    "print(\"vs adv:\", CFI_utils.bit_diff(victim_pattern_flatten, adv_pattern_flatten))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('py38': conda)",
   "language": "python",
   "name": "python385jvsc74a57bd08e59bf3d6a50a7077d8acb216dbc16c3106fe79f55067c17dc666a40d74b6917"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
