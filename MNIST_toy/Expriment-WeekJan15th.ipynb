{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import FeedforwardNeuralNetModel, TinyCNN, PatternClassifier\n",
    "import regularizer_losts as rl\n",
    "from torchvision import datasets, transforms\n",
    "from torch import optim\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import utils as CFI_utils\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "colors = sns.color_palette(\"tab10\")\n",
    "\n",
    "#Logging stuffs\n",
    "import logging\n",
    "import sys\n",
    "# Create logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create STDERR handler\n",
    "handler = logging.StreamHandler(sys.stderr)\n",
    "# ch.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create formatter and add it to the handler\n",
    "formatter = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "# Set STDERR handler as the only handler \n",
    "logger.handlers = [handler]\n",
    "\n",
    "\n",
    "#configs\n",
    "epochs = 3\n",
    "batch_size = 128\n",
    "test_batch_size = 10000\n",
    "use_cuda = True\n",
    "lr = 1\n",
    "log_interval = 100\n",
    "LOAD = True\n",
    "LOADPATH = 'TinyCNN15:36:27'\n",
    "\n",
    "#torch specific configs\n",
    "torch.manual_seed(1)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "train_kwargs = {'batch_size': batch_size}\n",
    "test_kwargs = {'batch_size': test_batch_size}\n",
    "\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                   'pin_memory': True,\n",
    "                   'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "### Big question: CFI for NN\n",
    "### Where are we?\n",
    "We want to do CFI for NN\n",
    "\n",
    "## Module 1: The number of activation patterns for a neural networks has to be way smaller than the size of the training/test set.\n",
    "- Why: If every input results in a different AP, there is no argument for using CFI.\n",
    "- Progress: with the current regularization effort, we have been able to reduce the number of AP to about 20k for 60k input.\n",
    "- Aim: Reduce to about few hundreds\n",
    "- __Tricky part__: Given a network of N layers, we can limit the number of activation patterns by only looking at k < N layers. Question: which k? \n",
    "\n",
    "## Module 2: All, or most, of the activation patterns of the test set has to be covered by the AP of the training set\n",
    "- Why: If the activation patterns in the test set are completely different from the training set, there would be too many false negative at the inferencing time\n",
    "- Progress: currently not the case. However, we hope that when we can significantly reduce the number of AP in the training set, it would be the case.\n",
    "\n",
    "## Module 3: Given an AP, how do we shield it?\n",
    "- Why: Assume we have very few AP, what do we do with it?\n",
    "- RQ1: Is it possible for inputs of different labels to have the same AP?\n",
    "- RQ2: If RQ1 is no, then is it possible to have adv. attack on the AP?\n",
    "- RQ3: \n",
    "\n",
    "### Backlog RQs: \n",
    "- BRQ1: Can we look at a random k entries in the weight instead of the first k?\n",
    "- BRQ2: What if we set all the small abs weight in the Pattern classifer to 0? what is the accuracy in that case?\n",
    "- BRQ3: Given an image and an l2-ball surrounding it, how many activation maps are there? (closely related to RQ1)\n",
    "- BRQ4: Given a simple attacking method (e.g, Fast gradient sign), build a dataset of activation maps of true and fake digits. Try to train a pattern classifier using that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init stuffs\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                          transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,\n",
    "                          transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "# model = FeedforwardNeuralNetModel(28*28, 128, 10).to(device)\n",
    "model = TinyCNN().to(device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('py38': conda)",
   "language": "python",
   "name": "python385jvsc74a57bd08e59bf3d6a50a7077d8acb216dbc16c3106fe79f55067c17dc666a40d74b6917"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
